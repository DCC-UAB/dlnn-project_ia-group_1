[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-24ddc0f5d75046c5622901739e7c5dd533143b0c8e959d652212380cedb1ea36.svg)](https://classroom.github.com/a/wT71nrpQ)
# Buisness Classification

In this GitHub repository, you can find all the necessary code of our implementation of a business classification algorithim on the [Con-Text dataset](https://staff.fnwi.uva.nl/s.karaoglu/datasetWeb/Dataset.html). Where given an image, our model is able to classify it to one of the different proposed classes.

In the dataset we can find 28 different business categories, which can be for instance: Bakery, Book Store, Motel and many others. As well as 24,255 images in total. 

Examples of images:

![image](https://github.com/DCC-UAB/dlnn-project_ia-group_1/assets/98542048/02d10d5f-dd18-4266-9641-ef07be7d3154)
![image](https://github.com/DCC-UAB/dlnn-project_ia-group_1/assets/98542048/73d9639e-8c40-4703-9d49-7756e8c2d5be)


## Repository Structure

#### Data folder:

The Data folder contains the output of the Preprocessing/Train_and_Test.py python file, which generates the Train.txt and Test.txt files. Each line in these files represents the name of a .jpg image and the corresponding business that appears in the image, encoded with a number.

The folder also includes the OCR output from the Preprocessing/OCR_eassyocr.py script. There are two OCR files available: one for the train set and another for the test set. Each line in these OCR files corresponds to an image in the dataset and contains the output generated by the EasyOCR tool. The lines containing a 0, are the ones where the eassy ocr tool has not find any text.

It's important to note that the number of lines in the OCR files matches the number of lines in the Train and Test txt files, ensuring that the OCR output is aligned with the respective images in the dataset.

#### Models

Contains the model used in this project

#### Preprocessing

Inside the preprocessing folder, you can find the code for separating the train and test sets and assigning labels to the images. Additionally, we have included the OCR_easyocr.py file, which performs Optical Character Recognition (OCR) on each image. It extracts the text and positional information of characters, enhancing the model's performance by utilizing relevant information about the businesses.

Here is an example of the easy_OCR output, where each line corresponds to an image. If a line contains a 0, it means there is no text in the image.

![image](https://github.com/DCC-UAB/dlnn-project_ia-group_1/assets/98542048/eba3b8b1-f7a8-4966-8321-216cafe262c0)

### Utils
Once the data is preprocessed, you can find the utils.py file in the OUR_utils folder. This file contains the train and test loaders, as well as the selection of the criterion and optimizer. 

Then we have the main.py file that sets various parameters such as learning rate and the number of classes. The train.py file is responsible for training the model, including the scheduler and other relevant aspects. And the test.py file, where the model's accuracy on the test set is calculated, and the weights of our model are saved in a file for future result visualization. The visualizations.ipynb file contains visualizations and the last evaluations of the model.
## Getting Started

### Prerequesites:
Firts you need to clone this repository in your machine.
```
https://github.com/DCC-UAB/dlnn-project_ia-group_1.git
```
In order to test the code you will need to install all the libraries in the .yml file. To make it easier you can create a new conda enviroment using the following comand
```
conda env create --file environment.yml
```
Aside from the .yml file we also are using the fasttext model to create the word embeddings. We have installed fasttext from their github directly using:
```
git clone https://github.com/facebookresearch/fastText.git
cd fastText
sudo pip install .
```
After installing the fastext model you also need to obtain the binary file needed to run the fasText model. To obtain this file you will need to run:
```
import fasttext.util
fasttext.util.download_model('en', if_exists='ignore')  # English
```
and a file called 'cc.en.300.bin' will be installed in the current folder.

### Executing the program
To run the program and replicate the model training, follow these steps:

1. Download the image dataset by clicking [here](http://isis-data.science.uva.nl/jvgemert/images.tar.gz). This dataset contains the necessary images for the training process.
2. After downloading the dataset.
3. Open the main.py file and locate the variables at the end of the file. These variables assign the paths to the images in your system.
4. Update the variables with the correct paths that correspond to the location of the installed image dataset on your system.
```
directory_test_train_files = 'path to the Data folder. The Data folder contains the files obtained when running the contents of the Preprocessing folder'
images_directory           = 'path to the .jpg images directory'             
fasttext_model_path        = 'path to the model containing the binary for the fastext model'
```
5. Save the main.py file with the updated paths.
6. Run the main.py file to execute the program and start the model training process.

By following these steps and ensuring the correct paths to the image dataset, you will be able to reproduce the model training successfully.

If you also want to try the preprocessing files you will need to install also the [Train_Test_partitions](http://isis-data.science.uva.nl/jvgemert/features.tar.gz). This directory contains the images "JPEGImages" and the labels, split into 3 random partitions (0,1,2). We follow the Pascal VOC format where each class has text file containing binary labels. Also text files containing the names of the train/test/train+test images are included.

If you want to use our already trained models, here you can found checkpoints for different models, you can download from here to use them for testing the model or training it a little more:

[Checkpoints of different models](https://drive.google.com/drive/folders/1yMg9uhjGs4E7EjoM9c3II3My5bD313TT?usp=sharing)
## Contributors

Authors: Nil Biescas, Xavi Soto, Jordi Longaron

Subject: Neural Networks and Deep Learning.

Degreee in Artificial Intelligence, 2n course.

UAB, 2023.

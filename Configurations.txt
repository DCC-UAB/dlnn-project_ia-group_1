Configuration 1 :
Using ADAMW with MultiStepLR, decaying the weihgt with gamma=0.1 at milestiones 15, 40.
At epoch 60 change to SGD with momentum = 0.9 and learning rate the same as ADAMW
config = dict(
    epochs=70,
    classes=28,
    batch_size=256,
    learning_rate=0.001,
    input_size=256,
    dim = 256,
    depth = 4,
    heads = 8,
    mlp_dim = 512,
    dataset="Con-Text dataset",
    architecture="ConTextTransformer")

Accuracy = 67.14 %

Configuration 2 :

Optimizer = Adam
Scheduler = MultiStepLR


Configuration 3 :

Adam wiht SGD and dropout:
Accuracy = 67.620109%
